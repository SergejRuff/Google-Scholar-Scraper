library(RSelenium)
source("code/proxies.R")
rm(list=ls())
library(rvest)
library(httr)
library(magrittr)
library(stringr)
library(Randomuseragent)
library(RSelenium)
source("code/proxies.R")
print(proxies)
rm(list=ls())
library(rvest)
library(httr)
library(magrittr)
library(stringr)
library(Randomuseragent)
library(RSelenium)
source("code/proxies.R")
rotate_proxy()
rm(list=ls())
library(rvest)
library(httr)
library(magrittr)
library(stringr)
library(Randomuseragent)
library(RSelenium)
source("code/proxies.R")
source("code/rotate_proxies.R")
rm(list=ls())
library(rvest)
library(httr)
library(magrittr)
library(stringr)
library(Randomuseragent)
library(RSelenium)
source("code/proxies.R")
source("code/rotate_proxies.R")
source("code/retry_request.R")
rm(list=ls())
library(rvest)
library(httr)
library(magrittr)
library(stringr)
library(Randomuseragent)
library(RSelenium)
source("code/proxies.R")
source("code/rotate_proxies.R")
source("code/retry_request.R")
source("code/count_elements.R")
remotes::install_github("tallguyjenks/gruvboxr")
gruvboxr::install_theme()
library(gruvboxr)
gruvboxr::install_theme()
View(scrape_gs)
View(test)
rm(list=ls())
gs_url_base <- "https://scholar.google.com/scholar?q="
rm(list=ls())
library(rvest)
library(httr)
library(magrittr)
library(stringr)
library(Randomuseragent)
library(RSelenium)
source("code/proxies.R")
source("code/rotate_proxies.R")
source("code/retry_request.R")
source("code/count_elements.R")
## This version of the function doesnt require the user to sepcify number of pages he wants to scrape.
## it scrapes all pages.
## might take longer, might cause http 429 error
## proxies taken from: https://scrapingant.com/free-proxies/
# this function goes through only the first 10 pages
# after taking a break count_elements returns 0 links causing empty lists
scrape_gs <- function(term,proxies,...) {
gs_url_base <- "https://scholar.google.com/scholar?q="
term <- gsub("^mmu-", "", term)
#term <- paste0("Virus AND intitle:",term)
print(paste0("intitle:",term))
best_proxy <- retry_request(paste0(gs_url_base, URLencode(paste0("intitle:",term))), proxies)
gs_page <- rvest::session(paste0(gs_url_base, URLencode(paste0("intitle:",term))), httr::use_proxy(url = best_proxy[[1]], port = best_proxy[[2]]))
#gs_page <- rvest::session(paste0(gs_url_base, URLencode(term)), httr::use_proxy(url = proxy_ip, port = proxy_port))
cat(paste("current link ->",paste0(gs_url_base, URLencode(paste0("intitle:",term)))),"\n")
# Read the HTML content of the Google Scholar search results page
page <- read_html(gs_page,user_agent=Randomuseragent::random_useragent())
# Count the number of links on the page
num_links <- count_elements(page)
message(num_links)
if (num_links == 0) {
return(NULL)
}
# Extract the entire page content as text
page_text <- as.character(page)
#print(page_text)
# Extract numbers within <span> tags from the text
numbers <- str_extract_all(page_text, "span>(\\d+)") %>%
unlist() %>%
str_replace("span>", "") %>%
as.integer()
# Find the maximum number
# Check if the count of elements is less than 10
if (num_links < 10) {
message("search-term has only 1 page")
max_number <- 1
} else {
max_number <- max(numbers, na.rm = TRUE)
}
if (is.infinite(max_number)) {
cat("Max number is -Inf. Trying with a new proxy.\n")
rotate_proxy()  # Rotate to a new proxy
return(scrape_gs(term, proxies))  # Retry scraping with a new proxy
}
cat(paste0(paste0("intitle:",term)," has ",max_number, " pages with results\n"))
# set httr config outside of function and use them inside ...; e.g.:
# useragent <- httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36") # spoof user agent
# proxy <- httr::use_proxy(url = "proxy.com", port = 8080, username = "dave", password = "pass", auth = "basic")
result_list <- list()
i <- 1
for (n_page in 0:(max_number- 1)*10) {  # gs page indexing starts with 0; there are 10 articles per page, see "?start=" param
gs_url <- paste0(gs_url_base, "&start=", n_page, "&q=", noquote(gsub("\\s+", "+", trimws(paste0("intitle:",term)))))
cat(paste(gs_url,"-> page:",n_page/10 + 1),"\n")
t0 <- Sys.time()
session <- rvest::session(gs_url, httr::use_proxy(url = best_proxy[[1]], port = best_proxy[[2]]))
#cat(paste0("current http status is ",session$response$status_code))
t1 <- Sys.time()
response_delay <- as.numeric(t1-t0)  # backing off time
wbpage <- rvest::read_html(session,user_agent=Randomuseragent::random_useragent())
#crawl_delay <- sample(2:10, 1)
crawl_delay <- 3
# Avoid HTTP error 429 due to too many requests - use crawl delay & back off
Sys.sleep(crawl_delay + 3*response_delay + runif(n = 1, min = 0.5, max = 1))
if((i %% 10) == 0) {  # sleep every 10 iterations
message("taking a break")
Sys.sleep(10 + 10*response_delay + runif(n = 1, min = 0, max = 1))
}
i <- i + 1
# Raw data
titles <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rt"))
#print(titles)
authors_years <- rvest::html_text(rvest::html_elements(wbpage, ".gs_a"))
part_abstracts <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rs"))
bottom_row_nodes <- rvest::html_elements(wbpage, ".gs_fl")
bottom_row_nodes <- bottom_row_nodes[!grepl("gs_ggs gs_fl", as.character(bottom_row_nodes), fixed = TRUE)] # exclude the ones with this tag, they are download links
bottom_row <- rvest::html_text(bottom_row_nodes)
# Processed data
authors <- gsub("^(.*?)\\W+-\\W+.*", "\\1", authors_years, perl = TRUE)
years <- gsub("^.*(\\d{4}).*", "\\1", authors_years, perl = TRUE)
citations <- strsplit(gsub("(?!^)(?=[[:upper:]])", " ", bottom_row, perl = TRUE), "  ")  # split on capital letter to get Number of citations link
citations <- lapply(citations, "[", 3)
n_citations <- suppressWarnings(as.numeric(sub("\\D*(\\d+).*", "\\1", citations)))
# Store in list
result_list <- append(
result_list,
list(
list(
page = n_page/10 + 1,
term = term,
title = titles,
authors = authors,
year = years,
n_citations = n_citations,
abstract = part_abstracts
)
)
)
}
# Return as data frame
result_df <- lapply(result_list, as.data.frame)
result_df <- as.data.frame(do.call(rbind, result_df))
result_df<- result_df[!grepl("\\b(?:[[:alpha:]]*omas?|cancer|leukemia)\\b", result_df$title, ignore.case = TRUE), ]
return(result_df)
}
testcsv <- read.csv("/home/sergej/Desktop/coding/viper/data/intercept/intersection_3.csv")
for (i in 24:length(testcsv$MiRNA)) {
mirna_name <- testcsv$MiRNA[[i]]
test <- scrape_gs(mirna_name, proxies)
# Check if test is NULL, which indicates that no links were found
if (is.null(test)) {
cat("No links found for:", mirna_name, "\n")
next  # Skip to the next iteration
}
output_file <- paste0("/home/sergej/Desktop/coding/scholar_scraper/output/", mirna_name, ".csv")
write.csv(test, output_file, row.names = FALSE)
cat("Scraped and saved:", mirna_name, "\n")
}
View(testcsv)
Sys.info()
Sys.info()["sysname"]
if(Sys.info()["sysname"]=="Linux")print("TRUE")
if(Sys.info()["sysname"]=="Linux")print("Es ist linux")
system("ls /")
system("which chromedriver")
test_ <- system("which chromedriver")
test_ <- system("which chromedriver",TRUE)
test_
test_ <- system("which pokemon",TRUE)
test_
test_[["status"]]
test_[[,"status"]]
test_[[1]]
test_[1]
test_ <- system("which chromedriver",TRUE)
test_[1]
test_ <- system("which pokemon",TRUE)
if(test[1]==NA)print("No entries")
if(test_[1]==NA)print("No entries")
if(test_[1]==NULL)print("No entries")
test_[1]
if(length(test_[1])==0)print("No entries")
length(test_[1]
length(test_[1])
test_[1]
if(is.na(test_[1])==0)print("No entries")
if(is.na(test_[1])==NA)print("No entries")
if(is.na(test_[1]))print("No entries")
# start server -> start Seleniumserver
rs_driver_object <- rsDriver(browser = "firefox",
geckover = "0.34.0",
verbose = FALSE, # verbose supresses messages while booting up
port =port4me::port4me()) # port4me
# start server -> start Seleniumserver
rs_driver_object <- rsDriver(browser = "firefox",
geckover = "0.34.0",
verbose = FALSE, # verbose supresses messages while booting up
port =port4me::port4me()) # port4me
library(RSelenium)
rm(list=ls())
## Rselium install
## You need: Java (JDK) -> Azul JDK works better with other packages in R like tabularize (java 8).
## https://www.azul.com/downloads/?package=jdk#zulu -> ubuntu: sudo apt install ./<package>.deb
## install.packages("Rselenium")
##  You need chromedriver and need to know your chromeversion(type chrome://version/ into chrome )
##  binman::list_versions("chromedriver") shows your available drivers.
##  make sure that teh two first digets match
##  driver: https://googlechromelabs.github.io/chrome-for-testing/
##  Firefox uses geckodriver
library(RSelenium)
library(tidyverse)
library(port4me) # allows to find/generate free ports
#library(wdman)
selenium(retcommand=T)
rm(list=ls())
## Rselium install
## You need: Java (JDK) -> Azul JDK works better with other packages in R like tabularize (java 8).
## https://www.azul.com/downloads/?package=jdk#zulu -> ubuntu: sudo apt install ./<package>.deb
## install.packages("Rselenium")
##  You need chromedriver and need to know your chromeversion(type chrome://version/ into chrome )
##  binman::list_versions("chromedriver") shows your available drivers.
##  make sure that teh two first digets match
##  driver: https://googlechromelabs.github.io/chrome-for-testing/
##  Firefox uses geckodriver
library(RSelenium)
library(tidyverse)
library(port4me) # allows to find/generate free ports
#library(wdman)
#selenium(retcommand=T)
# shows which driver versions are supported
#binman::list_versions("chromedriver")
binman::list_versions("geckodriver")
# start server -> start Seleniumserver
rs_driver_object <- rsDriver(browser = "firefox",
geckover = "0.34.0",
verbose = FALSE, # verbose supresses messages while booting up
port =port4me::port4me()) # port4me
?selenium
source("~/Desktop/coding/scholar_scraper/code/Rselium_attempts.R")
# show where every driver gets intalled to: important for driver
selenium_object <- selenium(retcommand=T,check = F)
selenium_object
# Get the remote driver (remDr) object
remDr <- rs_driver_object [["client"]]
source("~/Desktop/coding/scholar_scraper/code/Rselium_attempts.R")
source("~/Desktop/coding/scholar_scraper/code/Rselium_attempts.R")
testcsv <- read.csv("/home/sergej/Desktop/coding/viper/data/intercept/intersection_3.csv")
View(testcsv)
source("~/Desktop/coding/scholar_scraper/code/Rselium_attempts.R")
library(RSelenium)
library(tidyverse)
library(port4me)
library(wdman)
library(rvest)
library(stringr)
# Function to initialize and return a remote driver with specified options
initialize_driver <- function() {
rD <- rsDriver(browser = "firefox",
geckover = "0.34.0",
verbose = FALSE,
port = port4me::port4me())
rD[["client"]]
}
# Function to perform web scraping with delay and retry mechanism
scrape_with_retry <- function(remDr, search_terms, max_attempts = 3) {
attempt <- 1
while (attempt <= max_attempts) {
tryCatch({
remDr$navigate("https://scholar.google.com/")
search_box <- remDr$findElement("css", "#gs_hdr_tsi")
search_box$sendKeysToElement(list(search_terms, key = "enter"))
Sys.sleep(5) # Allow time for page to load
pages <- 2 # Number of pages to scrape
results <- data.frame()
for (page in 1:pages) {
page_source <- remDr$getPageSource()[[1]]
page_data <- extract_data(page_source)
results <- rbind(results, page_data)
next_button <- remDr$findElement("css", "#gs_n a")
if (length(next_button) == 0) {
break
} else {
next_button$clickElement()
Sys.sleep(5) # Allow time for page to load
}
}
return(results)
}, error = function(e) {
message("Attempt ", attempt, ": An error occurred: ", conditionMessage(e))
attempt <- attempt + 1
if (attempt <= max_attempts) {
message("Retrying...")
Sys.sleep(10) # Delay before retrying
}
})
}
message("Max attempts reached. Exiting...")
NULL
}
# Main scraping function
scrape_google_scholar <- function(search_terms) {
remDr <- initialize_driver()
search_results <- scrape_with_retry(remDr, search_terms)
remDr$close()
search_results
}
# Define your search terms
search_terms <- "miR-196-b"
# Execute the search and scrape the data
search_results <- scrape_google_scholar(search_terms)
source("~/Desktop/coding/scholar_scraper/code/search_google_scholar_nopage.R")
?remoteDriver
gs_url_base <- "https://scholar.google.com/scholar?q="
term <- gsub("^mmu-", "", term)
View(test)
View(test)
View(testcsv)
term = miR-196b
term = "miR-196b"
gs_url_base <- "https://scholar.google.com/scholar?q="
term <- gsub("^mmu-", "", term)
term <- gsub("-[^-]*$", "", term)
print(paste0("intitle:",term))
term ="mmu-miR-196b-5p"
gs_url_base <- "https://scholar.google.com/scholar?q="
term <- gsub("^mmu-", "", term)
term <- gsub("-[^-]*$", "", term)
print(paste0("intitle:",term))
website <- paste0("https://scholar.google.com/scholar?q=",URLencode(paste0("intitle:",term))) %>%
read_html()
a_elements <- website %>%
html_elements(css = "div.package > a")
a_elements <- website %>%
html_elements(css = "div.package > a")
a_elements <- website %>%
html_elements(css = "div.package > a")
View(a_elements)
a_elements
library(rvest)
# Define the search term
term <- "miR-196-b"
# Construct the URL
url <- paste0("https://scholar.google.com/scholar?q=", URLencode(paste0("intitle:", term)))
# Read the HTML content of the page
website <- read_html(url)
# Extract the titles of the search results
titles <- website %>%
html_nodes(".gs_rt a") %>%
html_text()
# View the titles
print(titles)
library(rvest)
# Define the search term
term <- "miR-196-b"
# Construct the URL
url <- paste0("https://scholar.google.com/scholar?q=", URLencode(paste0("intitle:", term)))
# Read the HTML content of the page
website <- read_html(url)
# Extract the titles of the search results
titles <- website %>%
html_nodes(".gs_rt a") %>%
html_text()
# View the titles
print(titles)
library(rvest)
# Define the search term
term <- "miR-196-b"
# Construct the URL
url <- paste0("https://scholar.google.com/scholar?q=", URLencode(paste0("intitle:", term)))
# Read the HTML content of the page
website <- read_html(url)
# Extract the titles of the search results
titles <- website %>%
html_nodes(".gs_rt a") %>%
html_text()
# View the titles
print(titles)
library(rvest)
# Define the search term
term <- "miR-196-b"
# Construct the URL
url <- paste0("https://scholar.google.com/scholar?q=", URLencode(paste0("intitle:", term)))
# Read the HTML content of the page
website <- read_html(url)
# Extract the titles of the search results
titles <- website %>%
html_nodes(".gs_rt a") %>%
html_text()
# View the titles
print(titles)
website %>%
html_element(css = "ul.pagination > li:nth-last-child(2)") %>%
html_text() %>%
as.numeric()
website %>%
html_element(css = "ul.pagination > li:nth-last-child(2)") %>%
html_text() %>%
as.numeric()
website %>%
html_element(css = "ul.pagination > li:nth-last-child(2)") %>%
html_text() %>%
as.numeric()
website %>%
html_element(css = "ul.pagination > li:nth-last-child(2)") %>%
html_text() %>%
as.numeric()
# View the HTML structure of the pagination element
website %>%
html_element(css = "ul.pagination > li:nth-last-child(2)") %>%
html_structure()
# View the HTML structure of the pagination element
pagination_html <- website %>%
html_element(css = "ul.pagination > li:nth-last-child(2)") %>%
html()
library(rvest)
# Define the search term
term <- "miR-196-b"
# Construct the URL
url <- paste0("https://scholar.google.com/scholar?q=", URLencode(paste0("intitle:", term)))
# Read the HTML content of the page
website <- read_html(url)
# Extract the total number of pages from the pagination section
pagination_elements <- website %>%
html_elements("ul > li")  # Adjust the CSS selector as needed to target the pagination elements
# Extract the page numbers from the pagination elements
page_numbers <- pagination_elements %>%
html_text(trim = TRUE) %>%
as.integer()
# Determine the maximum page number
max_page_number <- max(page_numbers, na.rm = TRUE)
print(max_page_number)
library(rvest)
# Define the search term
term <- "miR-196-b"
# Construct the URL
url <- paste0("https://scholar.google.com/scholar?q=", URLencode(paste0("intitle:", term)))
# Read the HTML content of the page
website <- read_html(url)
# Extract the total number of pages from the pagination section
pagination_elements <- website %>%
html_elements("ul > li")  # Adjust the CSS selector as needed to target the pagination elements
# Extract the page numbers from the pagination elements
page_numbers <- pagination_elements %>%
html_text(trim = TRUE) %>%
as.integer()
# Determine the maximum page number
max_page_number <- max(page_numbers, na.rm = TRUE)
print(max_page_number)
library(rvest)
# Define the search term
term <- "miR-196-b"
# Construct the URL
url <- paste0("https://scholar.google.com/scholar?q=", URLencode(paste0("intitle:", term)))
# Read the HTML content of the page
website <- read_html(url)
# Extract the total number of pages from the pagination section
pagination_elements <- website %>%
html_elements("ul > li")  # Adjust the CSS selector as needed to target the pagination elements
# Extract the page numbers from the pagination elements
page_numbers <- pagination_elements %>%
html_text(trim = TRUE) %>%
as.integer()
# Determine the maximum page number
max_page_number <- max(page_numbers, na.rm = TRUE)
print(max_page_number)
library(rvest)
# Define the search term
term <- "miR-196-b"
# Construct the URL
url <- paste0("https://scholar.google.com/scholar?q=", URLencode(paste0("intitle:", term)))
# Read the HTML content of the page
website <- read_html(url)
# Extract the total number of pages from the pagination section
pagination_elements <- website %>%
html_elements("ul > li")  # Adjust the CSS selector as needed to target the pagination elements
# Extract the page numbers from the pagination elements
page_numbers <- pagination_elements %>%
html_text(trim = TRUE) %>%
as.integer()
# Determine the maximum page number
max_page_number <- max(page_numbers, na.rm = TRUE)
print(max_page_number)
source("~/Desktop/coding/scholar_scraper/code/search_google_scholar_nopage.R")
warnings()
