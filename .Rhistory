## it scrapes all pages.
## might take longer, might cause http 429 error
scrape_gs <- function(term, crawl_delay, ...) {
library(rvest)
library(httr)
library(magrittr)
library(stringr)
gs_url_base <- "https://scholar.google.com/scholar?q="
gs_page <- paste0(gs_url_base, URLencode(term))
# Read the HTML content of the Google Scholar search results page
page <- read_html(gs_page)
# Extract the entire page content as text
page_text <- as.character(page)
print(page_text)
# Extract numbers within <span> tags from the text
numbers <- str_extract_all(page_text, "span>(\\d+)") %>%
unlist() %>%
str_replace("span>", "") %>%
as.integer()
# Find the maximum number
max_number <- max(numbers, na.rm = TRUE)
cat(paste0(term," has ",max_number, " pages with results"))
# set httr config outside of function and use them inside ...; e.g.:
# useragent <- httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36") # spoof user agent
# proxy <- httr::use_proxy(url = "proxy.com", port = 8080, username = "dave", password = "pass", auth = "basic")
result_list <- list()
i <- 1
for (n_page in 0:(max_number - 1)*10) {  # gs page indexing starts with 0; there are 10 articles per page, see "?start=" param
gs_url <- paste0(gs_url_base, "?start=", n_page, "&q=", noquote(gsub("\\s+", "+", trimws(term))))
t0 <- Sys.time()
session <- rvest::session(gs_url, ...)  # session$config$options$useragent
print(session$response$status_code)
t1 <- Sys.time()
response_delay <- as.numeric(t1-t0)  # backing off time
wbpage <- rvest::read_html(session)
# Avoid HTTP error 429 due to too many requests - use crawl delay & back off
Sys.sleep(crawl_delay + 3*response_delay + runif(n = 1, min = 0.5, max = 1))
if((i %% 10) == 0) {  # sleep every 10 iterations
message("taking a break")
Sys.sleep(10 + 10*response_delay + runif(n = 1, min = 0, max = 1))
}
i <- i + 1
# Raw data
titles <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rt"))
print(titles)
authors_years <- rvest::html_text(rvest::html_elements(wbpage, ".gs_a"))
part_abstracts <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rs"))
bottom_row_nodes <- rvest::html_elements(wbpage, ".gs_fl")
bottom_row_nodes <- bottom_row_nodes[!grepl("gs_ggs gs_fl", as.character(bottom_row_nodes), fixed = TRUE)] # exclude the ones with this tag, they are download links
bottom_row <- rvest::html_text(bottom_row_nodes)
# Processed data
authors <- gsub("^(.*?)\\W+-\\W+.*", "\\1", authors_years, perl = TRUE)
years <- gsub("^.*(\\d{4}).*", "\\1", authors_years, perl = TRUE)
citations <- strsplit(gsub("(?!^)(?=[[:upper:]])", " ", bottom_row, perl = TRUE), "  ")  # split on capital letter to get Number of citations link
citations <- lapply(citations, "[", 3)
n_citations <- suppressWarnings(as.numeric(sub("\\D*(\\d+).*", "\\1", citations)))
# Store in list
result_list <- append(
result_list,
list(
list(
page = n_page/10 + 1,
term = term,
title = titles,
authors = authors,
year = years,
n_citations = n_citations,
abstract = part_abstracts
)
)
)
}
# Return as data frame
result_df <- lapply(result_list, as.data.frame)
result_df <- as.data.frame(do.call(rbind, result_df))
result_df<- result_df[!grepl("\\b(?:[[:alpha:]]*omas?|cancer|leukemia)\\b", result_df$title, ignore.case = TRUE), ]
return(result_df)
}
test <- scrape_gs("mmu-miR-196b-5p",3)
rm(list=ls())
## This version of the function doesnt require the user to sepcify number of pages he wants to scrape.
## it scrapes all pages.
## might take longer, might cause http 429 error
scrape_gs <- function(term, crawl_delay, ...) {
library(rvest)
library(httr)
library(magrittr)
library(stringr)
gs_url_base <- "https://scholar.google.com/scholar?q="
gs_page <- paste0(gs_url_base, URLencode(term))
# Read the HTML content of the Google Scholar search results page
page <- read_html(gs_page)
# Extract the entire page content as text
page_text <- as.character(page)
print(page_text)
# Extract numbers within <span> tags from the text
numbers <- str_extract_all(page_text, "span>(\\d+)") %>%
unlist() %>%
str_replace("span>", "") %>%
as.integer()
# Find the maximum number
max_number <- max(numbers, na.rm = TRUE)
cat(paste0(term," has ",max_number, " pages with results"))
# set httr config outside of function and use them inside ...; e.g.:
# useragent <- httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36") # spoof user agent
# proxy <- httr::use_proxy(url = "proxy.com", port = 8080, username = "dave", password = "pass", auth = "basic")
result_list <- list()
i <- 1
for (n_page in 0:(3- 1)*10) {  # gs page indexing starts with 0; there are 10 articles per page, see "?start=" param
gs_url <- paste0(gs_url_base, "?start=", n_page, "&q=", noquote(gsub("\\s+", "+", trimws(term))))
t0 <- Sys.time()
session <- rvest::session(gs_url, ...)  # session$config$options$useragent
print(session$response$status_code)
t1 <- Sys.time()
response_delay <- as.numeric(t1-t0)  # backing off time
wbpage <- rvest::read_html(session)
# Avoid HTTP error 429 due to too many requests - use crawl delay & back off
Sys.sleep(crawl_delay + 3*response_delay + runif(n = 1, min = 0.5, max = 1))
if((i %% 10) == 0) {  # sleep every 10 iterations
message("taking a break")
Sys.sleep(10 + 10*response_delay + runif(n = 1, min = 0, max = 1))
}
i <- i + 1
# Raw data
titles <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rt"))
print(titles)
authors_years <- rvest::html_text(rvest::html_elements(wbpage, ".gs_a"))
part_abstracts <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rs"))
bottom_row_nodes <- rvest::html_elements(wbpage, ".gs_fl")
bottom_row_nodes <- bottom_row_nodes[!grepl("gs_ggs gs_fl", as.character(bottom_row_nodes), fixed = TRUE)] # exclude the ones with this tag, they are download links
bottom_row <- rvest::html_text(bottom_row_nodes)
# Processed data
authors <- gsub("^(.*?)\\W+-\\W+.*", "\\1", authors_years, perl = TRUE)
years <- gsub("^.*(\\d{4}).*", "\\1", authors_years, perl = TRUE)
citations <- strsplit(gsub("(?!^)(?=[[:upper:]])", " ", bottom_row, perl = TRUE), "  ")  # split on capital letter to get Number of citations link
citations <- lapply(citations, "[", 3)
n_citations <- suppressWarnings(as.numeric(sub("\\D*(\\d+).*", "\\1", citations)))
# Store in list
result_list <- append(
result_list,
list(
list(
page = n_page/10 + 1,
term = term,
title = titles,
authors = authors,
year = years,
n_citations = n_citations,
abstract = part_abstracts
)
)
)
}
# Return as data frame
result_df <- lapply(result_list, as.data.frame)
result_df <- as.data.frame(do.call(rbind, result_df))
result_df<- result_df[!grepl("\\b(?:[[:alpha:]]*omas?|cancer|leukemia)\\b", result_df$title, ignore.case = TRUE), ]
return(result_df)
}
test <- scrape_gs("mmu-miR-196b-5p",3)
library(rvest)
library(httr)
library(magrittr)
library(stringr)
gs_url_base <- "https://scholar.google.com/scholar?q="
gs_page <- paste0(gs_url_base, URLencode(mmu-miR-196b-5p))
library(rvest)
library(httr)
library(magrittr)
library(stringr)
gs_url_base <- "https://scholar.google.com/scholar?q="
gs_page <- paste0(gs_url_base, URLencode("mmu-miR-196b-5p"))
# Read the HTML content of the Google Scholar search results page
page <- read_html(gs_page)
# Extract the entire page content as text
page_text <- as.character(page)
#print(page_text)
# Extract numbers within <span> tags from the text
numbers <- str_extract_all(page_text, "span>(\\d+)") %>%
unlist() %>%
str_replace("span>", "") %>%
as.integer()
# Find the maximum number
max_number <- max(numbers, na.rm = TRUE)
cat(paste0("mmu-miR-196b-5p"," has ",max_number, " pages with results"))
str_extract_all(page_text, "span>(\\d+)")
str_extract_all(page_text, "span>.")
View(page)
xml_child(page, 2)
page$doc
page
page[2]
rvest::html_text(rvest::html_elements(page, ".gs_n"))
rvest::html_text(rvest::html_elements(page, ".gs_rt"))
url = gs_page
r <- read_html(url) %>%
html_node('body') %>%
html_text() %>%
toString()
data <- str_match_all(r,'var table_data = (.*?);')
data <- data[[1]][,2]  # string representation of list of lists
#step to convert string to object
rm(list=ls())
library(rvest)
search_query <- "mmu-miR-196b-5p"
search_url <- paste0("https://scholar.google.com/scholar?q=", URLencode(search_query))
search_page <- read_html(search_url)
titles <- search_page %>%
html_nodes(".gs_rt") %>%
html_text()
years <- search_page %>%
html_nodes(".gs_a") %>%
html_text()
years <- gsub("^.*(\\d{4}).*", "\\1", years, perl = TRUE)
# Combine titles and years into a data frame
search_results <- data.frame(Title = titles, Year = years)
# Filter out titles containing words ending with "oma" or "omas", as well as "cancer" and "leukemia"
search_results <- search_results[!grepl("\\b(?:[[:alpha:]]*omas?|cancer|leukemia)\\b", search_results$Title, ignore.case = TRUE), ]
# Print titles that are not filtered out
print(search_results$Title)
search_url
titles
install.packages("RepeatedHighDim")
for (n_page in 0:(max_number- 1)*10) {
print(n_page)
}
max_number=5
for (n_page in 0:(max_number- 1)*10) {
print(n_page)
}
rm(list=ls())
## This version of the function doesnt require the user to sepcify number of pages he wants to scrape.
## it scrapes all pages.
## might take longer, might cause http 429 error
scrape_gs <- function(term, crawl_delay, ...) {
library(rvest)
library(httr)
library(magrittr)
library(stringr)
gs_url_base <- "https://scholar.google.com/scholar?q="
gs_page <- paste0(gs_url_base, URLencode(term))
# Read the HTML content of the Google Scholar search results page
page <- read_html(gs_page)
# Extract the entire page content as text
page_text <- as.character(page)
#print(page_text)
# Extract numbers within <span> tags from the text
numbers <- str_extract_all(page_text, "span>(\\d+)") %>%
unlist() %>%
str_replace("span>", "") %>%
as.integer()
# Find the maximum number
max_number <- max(numbers, na.rm = TRUE)
cat(paste0(term," has ",max_number, " pages with results"))
# set httr config outside of function and use them inside ...; e.g.:
# useragent <- httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36") # spoof user agent
# proxy <- httr::use_proxy(url = "proxy.com", port = 8080, username = "dave", password = "pass", auth = "basic")
result_list <- list()
i <- 1
for (n_page in 0:(max_number- 1)*10) {  # gs page indexing starts with 0; there are 10 articles per page, see "?start=" param
gs_url <- paste0(gs_url_base, "?start=", n_page, "&q=", noquote(gsub("\\s+", "+", trimws(term))))
t0 <- Sys.time()
session <- rvest::session(gs_url, ...)  # session$config$options$useragent
cat(paste0("current http status is ",session$response$status_code))
t1 <- Sys.time()
response_delay <- as.numeric(t1-t0)  # backing off time
wbpage <- rvest::read_html(session)
# Avoid HTTP error 429 due to too many requests - use crawl delay & back off
Sys.sleep(crawl_delay + 3*response_delay + runif(n = 1, min = 0.5, max = 1))
if((i %% 10) == 0) {  # sleep every 10 iterations
message("taking a break")
Sys.sleep(10 + 10*response_delay + runif(n = 1, min = 0, max = 1))
}
i <- i + 1
# Raw data
titles <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rt"))
print(titles)
authors_years <- rvest::html_text(rvest::html_elements(wbpage, ".gs_a"))
part_abstracts <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rs"))
bottom_row_nodes <- rvest::html_elements(wbpage, ".gs_fl")
bottom_row_nodes <- bottom_row_nodes[!grepl("gs_ggs gs_fl", as.character(bottom_row_nodes), fixed = TRUE)] # exclude the ones with this tag, they are download links
bottom_row <- rvest::html_text(bottom_row_nodes)
# Processed data
authors <- gsub("^(.*?)\\W+-\\W+.*", "\\1", authors_years, perl = TRUE)
years <- gsub("^.*(\\d{4}).*", "\\1", authors_years, perl = TRUE)
citations <- strsplit(gsub("(?!^)(?=[[:upper:]])", " ", bottom_row, perl = TRUE), "  ")  # split on capital letter to get Number of citations link
citations <- lapply(citations, "[", 3)
n_citations <- suppressWarnings(as.numeric(sub("\\D*(\\d+).*", "\\1", citations)))
# Store in list
result_list <- append(
result_list,
list(
list(
page = n_page/10 + 1,
term = term,
title = titles,
authors = authors,
year = years,
n_citations = n_citations,
abstract = part_abstracts
)
)
)
}
# Return as data frame
result_df <- lapply(result_list, as.data.frame)
result_df <- as.data.frame(do.call(rbind, result_df))
result_df<- result_df[!grepl("\\b(?:[[:alpha:]]*omas?|cancer|leukemia)\\b", result_df$title, ignore.case = TRUE), ]
return(result_df)
}
test <- scrape_gs("mmu-miR-196b-5p",3)
rm(list=ls())
library(rvest)
search_query <- "mmu-miR-196b-5p"
search_url <- paste0("https://scholar.google.com/scholar?q=", URLencode(search_query))
search_page <- read_html(search_url)
rvest::html_text(rvest::html_elements(search_page, ".gs_n"))
search_page
titles <- search_page %>%
html_nodes(".gs_rt") %>%
html_text()
runif(1,1,3)
sample(c(1:5))
sample(1:5)
sample(5)
?runif
dunif(x, min = 0, max = 1, log = FALSE)
dunif(1, min = 0, max = 1, log = FALSE)
dunif(1, min = 0, max = 3, log = FALSE)
dunif(1, min = 0, max = 3, log = FALSE)
dunif(1, min = 0, max = 3, log = FALSE)
test <- c(1:5)
sample(test)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
sample(1:5, 1)
rm(list=ls())
## This version of the function doesnt require the user to sepcify number of pages he wants to scrape.
## it scrapes all pages.
## might take longer, might cause http 429 error
scrape_gs <- function(term,...) {
library(rvest)
library(httr)
library(magrittr)
library(stringr)
gs_url_base <- "https://scholar.google.com/scholar?q="
gs_page <- paste0(gs_url_base, URLencode(term))
# Read the HTML content of the Google Scholar search results page
page <- read_html(gs_page)
# Extract the entire page content as text
page_text <- as.character(page)
#print(page_text)
# Extract numbers within <span> tags from the text
numbers <- str_extract_all(page_text, "span>(\\d+)") %>%
unlist() %>%
str_replace("span>", "") %>%
as.integer()
# Find the maximum number
max_number <- max(numbers, na.rm = TRUE)
cat(paste0(term," has ",max_number, " pages with results"))
# set httr config outside of function and use them inside ...; e.g.:
# useragent <- httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36") # spoof user agent
# proxy <- httr::use_proxy(url = "proxy.com", port = 8080, username = "dave", password = "pass", auth = "basic")
result_list <- list()
i <- 1
for (n_page in 0:(max_number- 1)*10) {  # gs page indexing starts with 0; there are 10 articles per page, see "?start=" param
gs_url <- paste0(gs_url_base, "?start=", n_page, "&q=", noquote(gsub("\\s+", "+", trimws(term))))
t0 <- Sys.time()
session <- rvest::session(gs_url, ...)  # session$config$options$useragent
cat(paste0("current http status is ",session$response$status_code))
t1 <- Sys.time()
response_delay <- as.numeric(t1-t0)  # backing off time
wbpage <- rvest::read_html(session)
crawl_delay <- sample(1:3, 1)
# Avoid HTTP error 429 due to too many requests - use crawl delay & back off
Sys.sleep(crawl_delay + 3*response_delay + runif(n = 1, min = 0.5, max = 1))
if((i %% 10) == 0) {  # sleep every 10 iterations
message("taking a break")
Sys.sleep(10 + 10*response_delay + runif(n = 1, min = 0, max = 1))
}
i <- i + 1
# Raw data
titles <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rt"))
print(titles)
authors_years <- rvest::html_text(rvest::html_elements(wbpage, ".gs_a"))
part_abstracts <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rs"))
bottom_row_nodes <- rvest::html_elements(wbpage, ".gs_fl")
bottom_row_nodes <- bottom_row_nodes[!grepl("gs_ggs gs_fl", as.character(bottom_row_nodes), fixed = TRUE)] # exclude the ones with this tag, they are download links
bottom_row <- rvest::html_text(bottom_row_nodes)
# Processed data
authors <- gsub("^(.*?)\\W+-\\W+.*", "\\1", authors_years, perl = TRUE)
years <- gsub("^.*(\\d{4}).*", "\\1", authors_years, perl = TRUE)
citations <- strsplit(gsub("(?!^)(?=[[:upper:]])", " ", bottom_row, perl = TRUE), "  ")  # split on capital letter to get Number of citations link
citations <- lapply(citations, "[", 3)
n_citations <- suppressWarnings(as.numeric(sub("\\D*(\\d+).*", "\\1", citations)))
# Store in list
result_list <- append(
result_list,
list(
list(
page = n_page/10 + 1,
term = term,
title = titles,
authors = authors,
year = years,
n_citations = n_citations,
abstract = part_abstracts
)
)
)
}
# Return as data frame
result_df <- lapply(result_list, as.data.frame)
result_df <- as.data.frame(do.call(rbind, result_df))
result_df<- result_df[!grepl("\\b(?:[[:alpha:]]*omas?|cancer|leukemia)\\b", result_df$title, ignore.case = TRUE), ]
return(result_df)
}
test <- scrape_gs("mmu-miR-196b-5p")
rm(list=ls())
## This version of the function doesnt require the user to sepcify number of pages he wants to scrape.
## it scrapes all pages.
## might take longer, might cause http 429 error
scrape_gs <- function(term,...) {
library(rvest)
library(httr)
library(magrittr)
library(stringr)
gs_url_base <- "https://scholar.google.com/scholar?q="
gs_page <- paste0(gs_url_base, URLencode(term))
# Read the HTML content of the Google Scholar search results page
page <- read_html(gs_page)
# Extract the entire page content as text
page_text <- as.character(page)
#print(page_text)
# Extract numbers within <span> tags from the text
numbers <- str_extract_all(page_text, "span>(\\d+)") %>%
unlist() %>%
str_replace("span>", "") %>%
as.integer()
# Find the maximum number
max_number <- max(numbers, na.rm = TRUE)
cat(paste0(term," has ",max_number, " pages with results"))
# set httr config outside of function and use them inside ...; e.g.:
# useragent <- httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36") # spoof user agent
# proxy <- httr::use_proxy(url = "proxy.com", port = 8080, username = "dave", password = "pass", auth = "basic")
result_list <- list()
i <- 1
for (n_page in 0:(3- 1)*10) {  # gs page indexing starts with 0; there are 10 articles per page, see "?start=" param
gs_url <- paste0(gs_url_base, "?start=", n_page, "&q=", noquote(gsub("\\s+", "+", trimws(term))))
t0 <- Sys.time()
session <- rvest::session(gs_url, ...)  # session$config$options$useragent
cat(paste0("current http status is ",session$response$status_code))
t1 <- Sys.time()
response_delay <- as.numeric(t1-t0)  # backing off time
wbpage <- rvest::read_html(session)
crawl_delay <- sample(1:3, 1)
# Avoid HTTP error 429 due to too many requests - use crawl delay & back off
Sys.sleep(crawl_delay + 3*response_delay + runif(n = 1, min = 0.5, max = 1))
if((i %% 10) == 0) {  # sleep every 10 iterations
message("taking a break")
Sys.sleep(10 + 10*response_delay + runif(n = 1, min = 0, max = 1))
}
i <- i + 1
# Raw data
titles <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rt"))
print(titles)
authors_years <- rvest::html_text(rvest::html_elements(wbpage, ".gs_a"))
part_abstracts <- rvest::html_text(rvest::html_elements(wbpage, ".gs_rs"))
bottom_row_nodes <- rvest::html_elements(wbpage, ".gs_fl")
bottom_row_nodes <- bottom_row_nodes[!grepl("gs_ggs gs_fl", as.character(bottom_row_nodes), fixed = TRUE)] # exclude the ones with this tag, they are download links
bottom_row <- rvest::html_text(bottom_row_nodes)
# Processed data
authors <- gsub("^(.*?)\\W+-\\W+.*", "\\1", authors_years, perl = TRUE)
years <- gsub("^.*(\\d{4}).*", "\\1", authors_years, perl = TRUE)
citations <- strsplit(gsub("(?!^)(?=[[:upper:]])", " ", bottom_row, perl = TRUE), "  ")  # split on capital letter to get Number of citations link
citations <- lapply(citations, "[", 3)
n_citations <- suppressWarnings(as.numeric(sub("\\D*(\\d+).*", "\\1", citations)))
# Store in list
result_list <- append(
result_list,
list(
list(
page = n_page/10 + 1,
term = term,
title = titles,
authors = authors,
year = years,
n_citations = n_citations,
abstract = part_abstracts
)
)
)
}
# Return as data frame
result_df <- lapply(result_list, as.data.frame)
result_df <- as.data.frame(do.call(rbind, result_df))
result_df<- result_df[!grepl("\\b(?:[[:alpha:]]*omas?|cancer|leukemia)\\b", result_df$title, ignore.case = TRUE), ]
return(result_df)
}
test <- scrape_gs("mmu-miR-196b-5p")
